{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d84200",
   "metadata": {},
   "source": [
    "## Computational performance and real-time feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c95724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from utils import MIMONetDataset, DeepONetDataset, ChannelScaler\n",
    "from mimonet_drop import MIMONet_Drop\n",
    "from mimonet import MIMONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d836936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GH200 120GB\n"
     ]
    }
   ],
   "source": [
    "# Device + name\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dev_name = (torch.cuda.get_device_name(torch.cuda.current_device()) \n",
    "            if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", dev_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f7adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/projects/bcnx/kazumak2/MIMONet/HeatExchanger\"\n",
    "data_dir = os.path.join(working_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b3d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch1 shape: (1546, 2)\n",
      "Branch2 shape: (1546, 100)\n",
      "Selected output channels:\n",
      "pressure\n",
      "z-velocity\n",
      "y-velocity\n",
      "x-velocity\n",
      "velocity-magnitude\n",
      "Target shape: (1546, 3977, 7)\n",
      "Extracted target shape: (1546, 3977, 5)\n"
     ]
    }
   ],
   "source": [
    "# trunk dataset\n",
    "trunk_input = np.load(os.path.join(data_dir, \"share/trunk.npz\"))['trunk']\n",
    "\n",
    "# min-max scaling [-1, 1]\n",
    "trunk_input[:, 0] = 2 * (trunk_input[:, 0] - np.min(trunk_input[:, 0])) / (np.max(trunk_input[:, 0]) - np.min(trunk_input[:, 0])) - 1\n",
    "trunk_input[:, 1] = 2 * (trunk_input[:, 1] - np.min(trunk_input[:, 1])) / (np.max(trunk_input[:, 1]) - np.min(trunk_input[:, 1])) - 1\n",
    "\n",
    "# branch input dataset\n",
    "branch = np.load(os.path.join(data_dir, \"branch.npz\"))\n",
    "\n",
    "branch1 = branch['branch1']\n",
    "branch2 = branch['branch2']\n",
    "\n",
    "print(\"Branch1 shape:\", branch1.shape)\n",
    "print(\"Branch2 shape:\", branch2.shape)\n",
    "\n",
    "\n",
    "# create a dictionary for the output channel names\n",
    "# 0: turb-kinetic-energy\n",
    "# 1: pressure\n",
    "# 2: temperature\n",
    "# 3: z-velocity\n",
    "# 4: y-velocity\n",
    "# 5: x-velocity\n",
    "# 6: velocity-magnitude\n",
    "\n",
    "dict_channel = {\n",
    "    0: 'turb-kinetic-energy',\n",
    "    1: 'pressure',\n",
    "    2: 'temperature',\n",
    "    3: 'z-velocity',\n",
    "    4: 'y-velocity',\n",
    "    5: 'x-velocity',\n",
    "    6: 'velocity-magnitude'\n",
    "}\n",
    "\n",
    "# select the output channel\n",
    "target_channel = [1, 3, 4, 5, 6]\n",
    "\n",
    "# print the selected output channel names\n",
    "# target_label is used to store the names of the selected output channels for further processing (e.g., plotting)\n",
    "print(\"Selected output channels:\")\n",
    "target_label = []\n",
    "for channel in target_channel:\n",
    "    print(dict_channel[channel])\n",
    "    target_label.append(dict_channel[channel])    \n",
    "    \n",
    "# target dataset\n",
    "target = np.load(os.path.join(data_dir, \"target.npy\"))\n",
    "\n",
    "print(\"Target shape:\", target.shape)\n",
    "\n",
    "## extract the output channels\n",
    "## select the desired channels using the list (target_channel)\n",
    "target = target[..., target_channel]\n",
    "\n",
    "# print the shape of the extracted target\n",
    "print(\"Extracted target shape:\", target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4ae28",
   "metadata": {},
   "source": [
    "## Set Dataloader for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91564d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 1 dtype: torch.float32 shape: torch.Size([2])\n",
      "Branch 2 dtype: torch.float32 shape: torch.Size([100])\n",
      "Trunk dtype: torch.float32 shape: torch.Size([3977, 2])\n",
      "Target dtype: torch.float32 shape: torch.Size([3977, 5])\n"
     ]
    }
   ],
   "source": [
    "# dataset and dataloader\n",
    "dataset = MIMONetDataset(\n",
    "    [branch1, branch2],  # branch_data_list\n",
    "    trunk_input,         # trunk_data\n",
    "    target               # target_data\n",
    ")\n",
    "\n",
    "# Inspect a single sample from dataset\n",
    "sample = dataset[0]\n",
    "branch_data_sample, trunk_data_sample, target_data_sample = sample\n",
    "\n",
    "for i, b in enumerate(branch_data_sample):\n",
    "    print(f\"Branch {i+1} dtype:\", b.dtype, \"shape:\", b.shape)\n",
    "print(\"Trunk dtype:\", trunk_data_sample.dtype, \"shape:\", trunk_data_sample.shape)\n",
    "print(\"Target dtype:\", target_data_sample.dtype, \"shape:\", target_data_sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e89f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,762,052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_510165/1639123206.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('HeatExchanger/checkpoints/best_model_dropout.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture parameters\n",
    "dim = 256\n",
    "branch_input_dim1 = 2\n",
    "branch_input_dim2 = 100\n",
    "trunk_input_dim = 2\n",
    "\n",
    "# Define the model arguments for orig_MIMONet\n",
    "model_args = {\n",
    "    'branch_arch_list': [\n",
    "        [branch_input_dim1, 512, 512, 512, dim],\n",
    "        [branch_input_dim2, 512, 512, 512, dim]\n",
    "    ],\n",
    "    'trunk_arch': [trunk_input_dim, 256, 256, 256, dim],\n",
    "    'num_outputs': target.shape[-1] -1,  # number of output channels\n",
    "    'activation_fn': nn.ReLU,\n",
    "    'merge_type': 'mul',\n",
    "    'dropout_p': 0.1  # Dropout rate\n",
    "}\n",
    "\n",
    "model = MIMONet_Drop(**model_args)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print parameter count\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {num_params:,}\")\n",
    "\n",
    "# load the model\n",
    "model.load_state_dict(torch.load('HeatExchanger/checkpoints/best_model_dropout.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4c6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f0bc63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVML for power logging.\n",
      "Device name: NVIDIA GH200 120GB\n",
      "Warmup complete.\n",
      "Mean latency per inference: 4.307 ± 2.852 ms\n",
      "Throughput: 232.2 ± 153.8 samples/s\n",
      "Average Power: 177.11 ± 19.15 W\n",
      "Energy per inference: 0.778 ± 0.637 J\n",
      "Peak VRAM usage: 3451.3 MB (mean 3450.3 ± 0.3)\n",
      "\n",
      "Appended results to HeatExchanger/analysis/benchmark_all_devices.txt\n"
     ]
    }
   ],
   "source": [
    "import torch, time, numpy as np, subprocess\n",
    "\n",
    "# ======================================================\n",
    "# TRY NVML INIT (fallback to nvidia-smi if unavailable)\n",
    "# ======================================================\n",
    "use_nvml = False\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    use_nvml = True\n",
    "    print(\"Using NVML for power logging.\")\n",
    "except Exception as e:\n",
    "    print(\"NVML unavailable, using nvidia-smi fallback.\")\n",
    "    print(\"Reason:\", e)\n",
    "\n",
    "def read_power_mem():\n",
    "    \"\"\"Read GPU power (W) and memory (MB) using NVML or nvidia-smi.\"\"\"\n",
    "    if use_nvml:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W\n",
    "        mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1e6  # MB\n",
    "    else:\n",
    "        cmd = [\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=power.draw,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ]\n",
    "        try:\n",
    "            output = subprocess.check_output(cmd).decode().strip()\n",
    "            power, mem = map(float, output.split(\",\"))\n",
    "        except Exception:\n",
    "            power, mem = np.nan, np.nan\n",
    "    return power, mem\n",
    "\n",
    "# ======================================================\n",
    "# CONFIGURATION\n",
    "# ======================================================\n",
    "n_warmup = 10\n",
    "n_repeats = 1000\n",
    "print(\"Device name:\", dev_name)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ======================================================\n",
    "# WARMUP\n",
    "# ======================================================\n",
    "with torch.no_grad():\n",
    "    for i, (branch_data, trunk_data, target_data) in enumerate(dataloader):\n",
    "        branch_data = [b.to(device) for b in branch_data]\n",
    "        trunk_data = trunk_data.to(device)\n",
    "        _ = model(branch_data, trunk_data)\n",
    "        if i >= n_warmup:\n",
    "            break\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(\"Warmup complete.\")\n",
    "\n",
    "# ======================================================\n",
    "# TIMING + POWER LOGGING\n",
    "# ======================================================\n",
    "latencies, powers, mem_used = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (branch_data, trunk_data, target_data) in enumerate(dataloader):\n",
    "        branch_data = [b.to(device, non_blocking=True) for b in branch_data]\n",
    "        trunk_data = trunk_data.to(device, non_blocking=True)\n",
    "\n",
    "        # --- pre-inference metrics\n",
    "        power_pre, mem_pre = read_power_mem()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        pred = model(branch_data, trunk_data)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        # --- post-inference metrics\n",
    "        power_post, mem_post = read_power_mem()\n",
    "\n",
    "        latencies.append(t1 - t0)\n",
    "        powers.append(np.nanmean([power_pre, power_post]))\n",
    "        mem_used.append(np.nanmax([mem_pre, mem_post]))\n",
    "\n",
    "        if i >= n_repeats:\n",
    "            break\n",
    "\n",
    "# ======================================================\n",
    "# AGGREGATE RESULTS\n",
    "# ======================================================\n",
    "latencies, powers, mem_used = map(np.array, [latencies, powers, mem_used])\n",
    "\n",
    "mean_latency, std_latency = latencies.mean(), latencies.std()\n",
    "mean_power, std_power = powers.mean(), powers.std()\n",
    "energy_per_inf = latencies * powers\n",
    "mean_energy, std_energy = energy_per_inf.mean(), energy_per_inf.std()\n",
    "mean_mem, std_mem, peak_mem = mem_used.mean(), mem_used.std(), mem_used.max()\n",
    "\n",
    "# ======================================================\n",
    "# PRINT & SAVE\n",
    "# ======================================================\n",
    "print(f\"Mean latency per inference: {mean_latency*1000:.3f} ± {std_latency*1000:.3f} ms\")\n",
    "print(f\"Throughput: {1/mean_latency:.1f} ± {(std_latency/mean_latency**2):.1f} samples/s\")\n",
    "print(f\"Average Power: {mean_power:.2f} ± {std_power:.2f} W\")\n",
    "print(f\"Energy per inference: {mean_energy:.3f} ± {std_energy:.3f} J\")\n",
    "print(f\"Peak VRAM usage: {peak_mem:.1f} MB (mean {mean_mem:.1f} ± {std_mem:.1f})\")\n",
    "\n",
    "# --- save log ---\n",
    "output_file = \"HeatExchanger/analysis/benchmark_all_devices.txt\"\n",
    "with open(output_file, \"a\") as f:\n",
    "    f.write(f\"\\n{'='*50}\\n\")\n",
    "    f.write(f\"Device name: {dev_name}\\n\")\n",
    "    f.write(f\"Mean latency per inference: {mean_latency*1000:.3f} ± {std_latency*1000:.3f} ms\\n\")\n",
    "    f.write(f\"Throughput: {1/mean_latency:.1f} ± {(std_latency/mean_latency**2):.1f} samples/s\\n\")\n",
    "    f.write(f\"Average Power: {mean_power:.2f} ± {std_power:.2f} W\\n\")\n",
    "    f.write(f\"Energy per inference: {mean_energy:.3f} ± {std_energy:.3f} J\\n\")\n",
    "    f.write(f\"Peak VRAM usage: {peak_mem:.1f} MB (mean {mean_mem:.1f} ± {std_mem:.1f})\\n\")\n",
    "\n",
    "print(f\"\\nAppended results to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-env)",
   "language": "python",
   "name": "pytorch-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
