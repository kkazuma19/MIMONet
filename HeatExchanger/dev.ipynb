{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c7eb79",
   "metadata": {},
   "source": [
    "## Test Case III: Heat Exchanger (2D Plane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19563cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from utils import MIMONetDataset, DeepONetDataset, ChannelScaler\n",
    "from mimonet import MIMONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68626d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcf3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "working_dir = \"/projects/bcnx/kazumak2/MIMONet/HeatExchanger\"\n",
    "data_dir = os.path.join(working_dir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c3cb8",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662230f",
   "metadata": {},
   "source": [
    "### Load sharing parameters/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e26d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trunk dataset\n",
    "trunk_input = np.load(os.path.join(data_dir, \"share/trunk.npz\"))['trunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e053aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max scaling [-1, 1]\n",
    "trunk_input[:, 0] = 2 * (trunk_input[:, 0] - np.min(trunk_input[:, 0])) / (np.max(trunk_input[:, 0]) - np.min(trunk_input[:, 0])) - 1\n",
    "trunk_input[:, 1] = 2 * (trunk_input[:, 1] - np.min(trunk_input[:, 1])) / (np.max(trunk_input[:, 1]) - np.min(trunk_input[:, 1])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab5c36",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1bc97f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch1 shape: (1546, 2)\n",
      "Branch2 shape: (1546, 100)\n"
     ]
    }
   ],
   "source": [
    "# branch input dataset\n",
    "branch = np.load(os.path.join(data_dir, \"branch.npz\"))\n",
    "\n",
    "branch1 = branch['branch1']\n",
    "branch2 = branch['branch2']\n",
    "\n",
    "print(\"Branch1 shape:\", branch1.shape)\n",
    "print(\"Branch2 shape:\", branch2.shape)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(branch1))\n",
    "test_size = len(branch1) - train_size\n",
    "train_branch1, test_branch1 = branch1[:train_size], branch1[train_size:]\n",
    "train_branch2, test_branch2 = branch2[:train_size], branch2[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3f407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected output channels:\n",
      "pressure\n",
      "z-velocity\n",
      "y-velocity\n",
      "x-velocity\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary for the output channel names\n",
    "# 0: turb-kinetic-energy\n",
    "# 1: pressure\n",
    "# 2: temperature\n",
    "# 3: z-velocity\n",
    "# 4: y-velocity\n",
    "# 5: x-velocity\n",
    "# 6: velocity-magnitude\n",
    "\n",
    "dict_channel = {\n",
    "    0: 'turb-kinetic-energy',\n",
    "    1: 'pressure',\n",
    "    2: 'temperature',\n",
    "    3: 'z-velocity',\n",
    "    4: 'y-velocity',\n",
    "    5: 'x-velocity',\n",
    "    6: 'velocity-magnitude'\n",
    "}\n",
    "\n",
    "# select the output channel\n",
    "target_channel = [1, 3, 4, 5]\n",
    "\n",
    "# print the selected output channel names\n",
    "# target_label is used to store the names of the selected output channels for further processing (e.g., plotting)\n",
    "print(\"Selected output channels:\")\n",
    "target_label = []\n",
    "for channel in target_channel:\n",
    "    print(dict_channel[channel])\n",
    "    target_label.append(dict_channel[channel])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f02450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset shape before split: (1546, 3977, 4)\n",
      "Train target shape: (1236, 3977, 4)\n",
      "Test target shape: (310, 3977, 4)\n"
     ]
    }
   ],
   "source": [
    "# target dataset\n",
    "target = np.load(os.path.join(data_dir, \"target.npy\"))\n",
    "\n",
    "# extract the output channels\n",
    "target = target[:, :, target_channel ]  # select the first 7 channels\n",
    "print(\"Target dataset shape before split:\", target.shape)\n",
    "\n",
    "\n",
    "# split the target dataset into training and testing sets\n",
    "train_target = target[:train_size]\n",
    "test_target = target[train_size:]\n",
    "\n",
    "print(\"Train target shape:\", train_target.shape)\n",
    "print(\"Test target shape:\", test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85496e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of branch1: [  4.51454429 292.42944177]\n",
      "Std of branch1: [ 0.2615285  17.03323994]\n",
      "Mean of branch2: 12587.66968713018\n",
      "Std of branch2: 6302.709013835411\n"
     ]
    }
   ],
   "source": [
    "# (# train samples, 2) \n",
    "# get the mean and standard deviation of each channel\n",
    "mean_branch1 = np.mean(train_branch1, axis=0)\n",
    "std_branch1 = np.std(train_branch1, axis=0)\n",
    "\n",
    "print(\"Mean of branch1:\", mean_branch1)\n",
    "print(\"Std of branch1:\", std_branch1)\n",
    "\n",
    "# (# train samples, 100)\n",
    "mean_branch2 = np.mean(train_branch2)\n",
    "std_branch2 = np.std(train_branch2)\n",
    "\n",
    "print(\"Mean of branch2:\", mean_branch2)\n",
    "print(\"Std of branch2:\", std_branch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab08db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of normalized train_branch1: (1236, 2)\n",
      "Shape of normalized test_branch1: (310, 2)\n",
      "Shape of normalized train_branch2: (1236, 100)\n",
      "Shape of normalized test_branch2: (310, 100)\n"
     ]
    }
   ],
   "source": [
    "# normalize the branch data using the mean and std\n",
    "train_branch_1 = (train_branch1 - mean_branch1) / std_branch1\n",
    "test_branch_1 = (test_branch1 - mean_branch1) / std_branch1\n",
    "train_branch_2 = (train_branch2 - mean_branch2) / std_branch2\n",
    "test_branch_2 = (test_branch2 - mean_branch2) / std_branch2\n",
    "\n",
    "# print the shapes of the normalized data\n",
    "print(\"Shape of normalized train_branch1:\", train_branch_1.shape)\n",
    "print(\"Shape of normalized test_branch1:\", test_branch_1.shape)\n",
    "print(\"Shape of normalized train_branch2:\", train_branch_2.shape)\n",
    "print(\"Shape of normalized test_branch2:\", test_branch_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dddd92",
   "metadata": {},
   "source": [
    "### Scaling the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688e7a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of scaled train_target: (1236, 3977, 4)\n",
      "Shape of scaled test_target: (310, 3977, 4)\n"
     ]
    }
   ],
   "source": [
    "# scaling the target data\n",
    "'''  \n",
    "note: reverse the scaling for the target data\n",
    "train_target = scaler.inverse_transform(train_target_scaled)\n",
    "test_target = scaler.inverse_transform(test_target_scaled)\n",
    "'''\n",
    "scaler = ChannelScaler(method='minmax', feature_range=(-1, 1))\n",
    "scaler.fit(train_target)\n",
    "train_target_scaled = scaler.transform(train_target)\n",
    "test_target_scaled = scaler.transform(test_target)\n",
    "\n",
    "print(\"Shape of scaled train_target:\", train_target_scaled.shape)\n",
    "print(\"Shape of scaled test_target:\", test_target_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87ae6a",
   "metadata": {},
   "source": [
    "## Torch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf71b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset and dataloader\n",
    "test_dataset = MIMONetDataset(\n",
    "    [test_branch_1, test_branch_2],  # branch_data_list\n",
    "    trunk_input,                     # trunk_data\n",
    "    test_target_scaled               # target_data\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # set to 1 for testing\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a754121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MIMONetDataset(\n",
    "    [train_branch_1, train_branch_2],  # branch_data_list\n",
    "    trunk_input,                       # trunk_data\n",
    "    train_target_scaled                # target_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b2674",
   "metadata": {},
   "source": [
    "## MIMONet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2abb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "dim = 256\n",
    "branch_input_dim1 = 2\n",
    "branch_input_dim2 = 100\n",
    "trunk_input_dim = 2\n",
    "\n",
    "# Define the model arguments for orig_MIMONet\n",
    "model_args = {\n",
    "    'branch_arch_list': [\n",
    "        [branch_input_dim1, 512, 512, 512, dim],\n",
    "        [branch_input_dim2, 512, 512, 512, dim]\n",
    "    ],\n",
    "    'trunk_arch': [trunk_input_dim, 256, 256, 256, dim],\n",
    "    'num_outputs': target.shape[-1],  # number of output channels\n",
    "    'activation_fn': nn.ReLU,\n",
    "    'merge_type': 'mul',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eadbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae939d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler parameters\n",
    "scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_args={'mode': 'min', 'factor': 0.5, 'patience': 10,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "294932d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 1.00e-03, Train Loss: 0.028258, Val Loss: 0.007011\n",
      "Epoch 2, LR: 1.00e-03, Train Loss: 0.006811, Val Loss: 0.004199\n",
      "Epoch 3, LR: 1.00e-03, Train Loss: 0.006016, Val Loss: 0.003918\n",
      "Epoch 4, LR: 1.00e-03, Train Loss: 0.004786, Val Loss: 0.003113\n",
      "Epoch 5, LR: 1.00e-03, Train Loss: 0.004624, Val Loss: 0.003474\n",
      "Epoch 6, LR: 1.00e-03, Train Loss: 0.004475, Val Loss: 0.002480\n",
      "Epoch 7, LR: 1.00e-03, Train Loss: 0.003389, Val Loss: 0.002348\n",
      "Epoch 8, LR: 1.00e-03, Train Loss: 0.004654, Val Loss: 0.003780\n",
      "Epoch 9, LR: 1.00e-03, Train Loss: 0.003680, Val Loss: 0.002154\n",
      "Epoch 10, LR: 1.00e-03, Train Loss: 0.003066, Val Loss: 0.002664\n",
      "Epoch 11, LR: 1.00e-03, Train Loss: 0.002538, Val Loss: 0.004281\n",
      "Epoch 12, LR: 1.00e-03, Train Loss: 0.003189, Val Loss: 0.002912\n",
      "Epoch 13, LR: 1.00e-03, Train Loss: 0.002797, Val Loss: 0.001870\n",
      "Epoch 14, LR: 1.00e-03, Train Loss: 0.002163, Val Loss: 0.002584\n",
      "Epoch 15, LR: 1.00e-03, Train Loss: 0.002121, Val Loss: 0.001879\n",
      "Epoch 16, LR: 1.00e-03, Train Loss: 0.001667, Val Loss: 0.004388\n",
      "Epoch 17, LR: 1.00e-03, Train Loss: 0.001508, Val Loss: 0.001235\n",
      "Epoch 18, LR: 1.00e-03, Train Loss: 0.001448, Val Loss: 0.002047\n",
      "Epoch 19, LR: 1.00e-03, Train Loss: 0.002984, Val Loss: 0.001466\n",
      "Epoch 20, LR: 1.00e-03, Train Loss: 0.001031, Val Loss: 0.000888\n",
      "Epoch 21, LR: 1.00e-03, Train Loss: 0.001302, Val Loss: 0.000611\n",
      "Epoch 22, LR: 1.00e-03, Train Loss: 0.001431, Val Loss: 0.002211\n",
      "Epoch 23, LR: 1.00e-03, Train Loss: 0.001154, Val Loss: 0.000773\n",
      "Epoch 24, LR: 1.00e-03, Train Loss: 0.000844, Val Loss: 0.000778\n",
      "Epoch 25, LR: 1.00e-03, Train Loss: 0.000962, Val Loss: 0.000585\n",
      "Epoch 26, LR: 1.00e-03, Train Loss: 0.001360, Val Loss: 0.001067\n",
      "Epoch 27, LR: 1.00e-03, Train Loss: 0.000710, Val Loss: 0.000613\n",
      "Epoch 28, LR: 1.00e-03, Train Loss: 0.001152, Val Loss: 0.000443\n",
      "Epoch 29, LR: 1.00e-03, Train Loss: 0.000914, Val Loss: 0.000809\n",
      "Epoch 30, LR: 1.00e-03, Train Loss: 0.001220, Val Loss: 0.000479\n",
      "Epoch 31, LR: 1.00e-03, Train Loss: 0.001085, Val Loss: 0.001964\n",
      "Epoch 32, LR: 1.00e-03, Train Loss: 0.000814, Val Loss: 0.000563\n",
      "Epoch 33, LR: 1.00e-03, Train Loss: 0.001154, Val Loss: 0.000378\n",
      "Epoch 34, LR: 1.00e-03, Train Loss: 0.000778, Val Loss: 0.000501\n",
      "Epoch 35, LR: 1.00e-03, Train Loss: 0.000743, Val Loss: 0.000728\n",
      "Epoch 36, LR: 1.00e-03, Train Loss: 0.001173, Val Loss: 0.000434\n",
      "Epoch 37, LR: 1.00e-03, Train Loss: 0.000693, Val Loss: 0.000393\n",
      "Epoch 38, LR: 1.00e-03, Train Loss: 0.001038, Val Loss: 0.000619\n",
      "Epoch 39, LR: 1.00e-03, Train Loss: 0.001039, Val Loss: 0.002033\n",
      "Epoch 40, LR: 1.00e-03, Train Loss: 0.000671, Val Loss: 0.002893\n",
      "Epoch 41, LR: 1.00e-03, Train Loss: 0.000897, Val Loss: 0.000410\n",
      "Epoch 42, LR: 1.00e-03, Train Loss: 0.000799, Val Loss: 0.000397\n",
      "Epoch 43, LR: 1.00e-03, Train Loss: 0.000506, Val Loss: 0.000631\n",
      "Epoch 44, LR: 5.00e-04, Train Loss: 0.001060, Val Loss: 0.002463\n",
      "Epoch 45, LR: 5.00e-04, Train Loss: 0.000339, Val Loss: 0.000265\n",
      "Epoch 46, LR: 5.00e-04, Train Loss: 0.000245, Val Loss: 0.000239\n",
      "Epoch 47, LR: 5.00e-04, Train Loss: 0.000230, Val Loss: 0.000226\n",
      "Epoch 48, LR: 5.00e-04, Train Loss: 0.000252, Val Loss: 0.000264\n",
      "Epoch 49, LR: 5.00e-04, Train Loss: 0.000272, Val Loss: 0.000241\n",
      "Epoch 50, LR: 5.00e-04, Train Loss: 0.000298, Val Loss: 0.000588\n",
      "Epoch 51, LR: 5.00e-04, Train Loss: 0.000397, Val Loss: 0.000414\n",
      "Epoch 52, LR: 5.00e-04, Train Loss: 0.000312, Val Loss: 0.000229\n",
      "Epoch 53, LR: 5.00e-04, Train Loss: 0.000412, Val Loss: 0.000197\n",
      "Epoch 54, LR: 5.00e-04, Train Loss: 0.000277, Val Loss: 0.000242\n",
      "Epoch 55, LR: 5.00e-04, Train Loss: 0.000328, Val Loss: 0.000214\n",
      "Epoch 56, LR: 5.00e-04, Train Loss: 0.000563, Val Loss: 0.000198\n",
      "Epoch 57, LR: 5.00e-04, Train Loss: 0.000192, Val Loss: 0.000350\n",
      "Epoch 58, LR: 5.00e-04, Train Loss: 0.000381, Val Loss: 0.000190\n",
      "Epoch 59, LR: 5.00e-04, Train Loss: 0.000230, Val Loss: 0.000209\n",
      "Epoch 60, LR: 5.00e-04, Train Loss: 0.000352, Val Loss: 0.000194\n",
      "Epoch 61, LR: 5.00e-04, Train Loss: 0.000297, Val Loss: 0.000283\n",
      "Epoch 62, LR: 5.00e-04, Train Loss: 0.000323, Val Loss: 0.000372\n",
      "Epoch 63, LR: 5.00e-04, Train Loss: 0.000279, Val Loss: 0.000173\n",
      "Epoch 64, LR: 5.00e-04, Train Loss: 0.000381, Val Loss: 0.000605\n",
      "Epoch 65, LR: 5.00e-04, Train Loss: 0.000353, Val Loss: 0.000203\n",
      "Epoch 66, LR: 5.00e-04, Train Loss: 0.000205, Val Loss: 0.000194\n",
      "Epoch 67, LR: 5.00e-04, Train Loss: 0.000288, Val Loss: 0.000197\n",
      "Epoch 68, LR: 5.00e-04, Train Loss: 0.000348, Val Loss: 0.000181\n",
      "Epoch 69, LR: 5.00e-04, Train Loss: 0.000263, Val Loss: 0.000204\n",
      "Epoch 70, LR: 5.00e-04, Train Loss: 0.000258, Val Loss: 0.000540\n",
      "Epoch 71, LR: 5.00e-04, Train Loss: 0.000334, Val Loss: 0.000216\n",
      "Epoch 72, LR: 5.00e-04, Train Loss: 0.000216, Val Loss: 0.000242\n",
      "Epoch 73, LR: 5.00e-04, Train Loss: 0.000317, Val Loss: 0.000252\n",
      "Epoch 74, LR: 2.50e-04, Train Loss: 0.000262, Val Loss: 0.000252\n",
      "Epoch 75, LR: 2.50e-04, Train Loss: 0.000140, Val Loss: 0.000140\n",
      "Epoch 76, LR: 2.50e-04, Train Loss: 0.000134, Val Loss: 0.000134\n",
      "Epoch 77, LR: 2.50e-04, Train Loss: 0.000132, Val Loss: 0.000131\n",
      "Epoch 78, LR: 2.50e-04, Train Loss: 0.000136, Val Loss: 0.000131\n",
      "Epoch 79, LR: 2.50e-04, Train Loss: 0.000145, Val Loss: 0.000147\n",
      "Epoch 80, LR: 2.50e-04, Train Loss: 0.000161, Val Loss: 0.000144\n",
      "Epoch 81, LR: 2.50e-04, Train Loss: 0.000174, Val Loss: 0.000160\n",
      "Epoch 82, LR: 2.50e-04, Train Loss: 0.000179, Val Loss: 0.000480\n",
      "Epoch 83, LR: 2.50e-04, Train Loss: 0.000200, Val Loss: 0.000124\n",
      "Epoch 84, LR: 2.50e-04, Train Loss: 0.000147, Val Loss: 0.000162\n",
      "Epoch 85, LR: 2.50e-04, Train Loss: 0.000139, Val Loss: 0.000135\n",
      "Epoch 86, LR: 2.50e-04, Train Loss: 0.000158, Val Loss: 0.001897\n",
      "Epoch 87, LR: 2.50e-04, Train Loss: 0.000197, Val Loss: 0.000119\n",
      "Epoch 88, LR: 2.50e-04, Train Loss: 0.000160, Val Loss: 0.000183\n",
      "Epoch 89, LR: 2.50e-04, Train Loss: 0.000189, Val Loss: 0.000210\n",
      "Epoch 90, LR: 2.50e-04, Train Loss: 0.000148, Val Loss: 0.000152\n",
      "Epoch 91, LR: 2.50e-04, Train Loss: 0.000140, Val Loss: 0.000155\n",
      "Epoch 92, LR: 2.50e-04, Train Loss: 0.000139, Val Loss: 0.000153\n",
      "Epoch 93, LR: 2.50e-04, Train Loss: 0.000154, Val Loss: 0.000116\n",
      "Epoch 94, LR: 2.50e-04, Train Loss: 0.000156, Val Loss: 0.000186\n",
      "Epoch 95, LR: 2.50e-04, Train Loss: 0.000170, Val Loss: 0.000162\n",
      "Epoch 96, LR: 2.50e-04, Train Loss: 0.000151, Val Loss: 0.000284\n",
      "Epoch 97, LR: 2.50e-04, Train Loss: 0.000155, Val Loss: 0.000259\n",
      "Epoch 98, LR: 2.50e-04, Train Loss: 0.000143, Val Loss: 0.000134\n",
      "Epoch 99, LR: 2.50e-04, Train Loss: 0.000162, Val Loss: 0.000143\n",
      "Epoch 100, LR: 2.50e-04, Train Loss: 0.000143, Val Loss: 0.000127\n",
      "Epoch 101, LR: 2.50e-04, Train Loss: 0.000148, Val Loss: 0.000157\n",
      "Epoch 102, LR: 2.50e-04, Train Loss: 0.000191, Val Loss: 0.000125\n",
      "Epoch 103, LR: 2.50e-04, Train Loss: 0.000129, Val Loss: 0.000190\n",
      "Epoch 104, LR: 1.25e-04, Train Loss: 0.000131, Val Loss: 0.000135\n",
      "Epoch 105, LR: 1.25e-04, Train Loss: 0.000099, Val Loss: 0.000097\n",
      "Epoch 106, LR: 1.25e-04, Train Loss: 0.000098, Val Loss: 0.000102\n",
      "Epoch 107, LR: 1.25e-04, Train Loss: 0.000097, Val Loss: 0.000103\n",
      "Epoch 108, LR: 1.25e-04, Train Loss: 0.000099, Val Loss: 0.000099\n",
      "Epoch 109, LR: 1.25e-04, Train Loss: 0.000099, Val Loss: 0.000098\n",
      "Epoch 110, LR: 1.25e-04, Train Loss: 0.000107, Val Loss: 0.000110\n",
      "Epoch 111, LR: 1.25e-04, Train Loss: 0.000111, Val Loss: 0.000105\n",
      "Epoch 112, LR: 1.25e-04, Train Loss: 0.000106, Val Loss: 0.000100\n",
      "Epoch 113, LR: 1.25e-04, Train Loss: 0.000103, Val Loss: 0.000151\n",
      "Epoch 114, LR: 1.25e-04, Train Loss: 0.000110, Val Loss: 0.000105\n",
      "Epoch 115, LR: 1.25e-04, Train Loss: 0.000106, Val Loss: 0.000092\n",
      "Epoch 116, LR: 1.25e-04, Train Loss: 0.000109, Val Loss: 0.000100\n",
      "Epoch 117, LR: 1.25e-04, Train Loss: 0.000117, Val Loss: 0.000093\n",
      "Epoch 118, LR: 1.25e-04, Train Loss: 0.000096, Val Loss: 0.000098\n",
      "Epoch 119, LR: 1.25e-04, Train Loss: 0.000107, Val Loss: 0.000095\n",
      "Epoch 120, LR: 1.25e-04, Train Loss: 0.000100, Val Loss: 0.000111\n",
      "Epoch 121, LR: 1.25e-04, Train Loss: 0.000106, Val Loss: 0.000103\n",
      "Epoch 122, LR: 1.25e-04, Train Loss: 0.000126, Val Loss: 0.000135\n",
      "Epoch 123, LR: 1.25e-04, Train Loss: 0.000092, Val Loss: 0.000102\n",
      "Epoch 124, LR: 1.25e-04, Train Loss: 0.000099, Val Loss: 0.000098\n",
      "Epoch 125, LR: 1.25e-04, Train Loss: 0.000098, Val Loss: 0.000093\n",
      "Epoch 126, LR: 6.25e-05, Train Loss: 0.000098, Val Loss: 0.000156\n",
      "Epoch 127, LR: 6.25e-05, Train Loss: 0.000086, Val Loss: 0.000088\n",
      "Epoch 128, LR: 6.25e-05, Train Loss: 0.000084, Val Loss: 0.000084\n",
      "Epoch 129, LR: 6.25e-05, Train Loss: 0.000084, Val Loss: 0.000085\n",
      "Epoch 130, LR: 6.25e-05, Train Loss: 0.000083, Val Loss: 0.000090\n",
      "Epoch 131, LR: 6.25e-05, Train Loss: 0.000085, Val Loss: 0.000089\n",
      "Epoch 132, LR: 6.25e-05, Train Loss: 0.000085, Val Loss: 0.000082\n",
      "Epoch 133, LR: 6.25e-05, Train Loss: 0.000087, Val Loss: 0.000085\n",
      "Epoch 134, LR: 6.25e-05, Train Loss: 0.000085, Val Loss: 0.000095\n",
      "Epoch 135, LR: 6.25e-05, Train Loss: 0.000085, Val Loss: 0.000093\n",
      "Epoch 136, LR: 6.25e-05, Train Loss: 0.000083, Val Loss: 0.000083\n",
      "Epoch 137, LR: 6.25e-05, Train Loss: 0.000084, Val Loss: 0.000083\n",
      "Epoch 138, LR: 6.25e-05, Train Loss: 0.000086, Val Loss: 0.000088\n",
      "Epoch 139, LR: 6.25e-05, Train Loss: 0.000082, Val Loss: 0.000082\n",
      "Epoch 140, LR: 6.25e-05, Train Loss: 0.000085, Val Loss: 0.000081\n",
      "Epoch 141, LR: 6.25e-05, Train Loss: 0.000085, Val Loss: 0.000084\n",
      "Epoch 142, LR: 6.25e-05, Train Loss: 0.000084, Val Loss: 0.000088\n",
      "Epoch 143, LR: 6.25e-05, Train Loss: 0.000082, Val Loss: 0.000080\n",
      "Epoch 144, LR: 6.25e-05, Train Loss: 0.000081, Val Loss: 0.000078\n",
      "Epoch 145, LR: 6.25e-05, Train Loss: 0.000081, Val Loss: 0.000080\n",
      "Epoch 146, LR: 6.25e-05, Train Loss: 0.000083, Val Loss: 0.000089\n",
      "Epoch 147, LR: 6.25e-05, Train Loss: 0.000079, Val Loss: 0.000077\n",
      "Epoch 148, LR: 6.25e-05, Train Loss: 0.000080, Val Loss: 0.000081\n",
      "Epoch 149, LR: 6.25e-05, Train Loss: 0.000081, Val Loss: 0.000084\n",
      "Epoch 150, LR: 6.25e-05, Train Loss: 0.000080, Val Loss: 0.000079\n",
      "Epoch 151, LR: 6.25e-05, Train Loss: 0.000079, Val Loss: 0.000085\n",
      "Epoch 152, LR: 6.25e-05, Train Loss: 0.000084, Val Loss: 0.000084\n",
      "Epoch 153, LR: 6.25e-05, Train Loss: 0.000077, Val Loss: 0.000079\n",
      "Epoch 154, LR: 6.25e-05, Train Loss: 0.000078, Val Loss: 0.000088\n",
      "Epoch 155, LR: 6.25e-05, Train Loss: 0.000080, Val Loss: 0.000080\n",
      "Epoch 156, LR: 6.25e-05, Train Loss: 0.000077, Val Loss: 0.000078\n",
      "Epoch 157, LR: 6.25e-05, Train Loss: 0.000079, Val Loss: 0.000091\n",
      "Epoch 158, LR: 6.25e-05, Train Loss: 0.000080, Val Loss: 0.000075\n",
      "Epoch 159, LR: 6.25e-05, Train Loss: 0.000078, Val Loss: 0.000080\n",
      "Epoch 160, LR: 6.25e-05, Train Loss: 0.000078, Val Loss: 0.000081\n",
      "Epoch 161, LR: 6.25e-05, Train Loss: 0.000078, Val Loss: 0.000077\n",
      "Epoch 162, LR: 6.25e-05, Train Loss: 0.000077, Val Loss: 0.000078\n",
      "Epoch 163, LR: 6.25e-05, Train Loss: 0.000077, Val Loss: 0.000079\n",
      "Epoch 164, LR: 6.25e-05, Train Loss: 0.000076, Val Loss: 0.000077\n",
      "Epoch 165, LR: 6.25e-05, Train Loss: 0.000077, Val Loss: 0.000074\n",
      "Epoch 166, LR: 6.25e-05, Train Loss: 0.000076, Val Loss: 0.000080\n",
      "Epoch 167, LR: 6.25e-05, Train Loss: 0.000076, Val Loss: 0.000073\n",
      "Epoch 168, LR: 6.25e-05, Train Loss: 0.000076, Val Loss: 0.000092\n",
      "Epoch 169, LR: 6.25e-05, Train Loss: 0.000074, Val Loss: 0.000082\n",
      "Epoch 170, LR: 6.25e-05, Train Loss: 0.000076, Val Loss: 0.000074\n",
      "Epoch 171, LR: 6.25e-05, Train Loss: 0.000078, Val Loss: 0.000074\n",
      "Epoch 172, LR: 6.25e-05, Train Loss: 0.000072, Val Loss: 0.000083\n",
      "Epoch 173, LR: 6.25e-05, Train Loss: 0.000073, Val Loss: 0.000071\n",
      "Epoch 174, LR: 6.25e-05, Train Loss: 0.000078, Val Loss: 0.000076\n",
      "Epoch 175, LR: 6.25e-05, Train Loss: 0.000074, Val Loss: 0.000073\n",
      "Epoch 176, LR: 6.25e-05, Train Loss: 0.000083, Val Loss: 0.000074\n",
      "Epoch 177, LR: 6.25e-05, Train Loss: 0.000070, Val Loss: 0.000078\n",
      "Epoch 178, LR: 6.25e-05, Train Loss: 0.000072, Val Loss: 0.000117\n",
      "Epoch 179, LR: 6.25e-05, Train Loss: 0.000073, Val Loss: 0.000081\n",
      "Epoch 180, LR: 6.25e-05, Train Loss: 0.000077, Val Loss: 0.000076\n",
      "Epoch 181, LR: 6.25e-05, Train Loss: 0.000071, Val Loss: 0.000071\n",
      "Epoch 182, LR: 6.25e-05, Train Loss: 0.000072, Val Loss: 0.000071\n",
      "Epoch 183, LR: 6.25e-05, Train Loss: 0.000075, Val Loss: 0.000078\n",
      "Epoch 184, LR: 6.25e-05, Train Loss: 0.000074, Val Loss: 0.000078\n",
      "Epoch 185, LR: 6.25e-05, Train Loss: 0.000071, Val Loss: 0.000071\n",
      "Epoch 186, LR: 6.25e-05, Train Loss: 0.000075, Val Loss: 0.000094\n",
      "Epoch 187, LR: 6.25e-05, Train Loss: 0.000074, Val Loss: 0.000070\n",
      "Epoch 188, LR: 6.25e-05, Train Loss: 0.000069, Val Loss: 0.000080\n",
      "Epoch 189, LR: 6.25e-05, Train Loss: 0.000074, Val Loss: 0.000068\n",
      "Epoch 190, LR: 6.25e-05, Train Loss: 0.000073, Val Loss: 0.000071\n",
      "Epoch 191, LR: 6.25e-05, Train Loss: 0.000071, Val Loss: 0.000070\n",
      "Epoch 192, LR: 6.25e-05, Train Loss: 0.000074, Val Loss: 0.000100\n",
      "Epoch 193, LR: 6.25e-05, Train Loss: 0.000070, Val Loss: 0.000070\n",
      "Epoch 194, LR: 6.25e-05, Train Loss: 0.000069, Val Loss: 0.000083\n",
      "Epoch 195, LR: 6.25e-05, Train Loss: 0.000071, Val Loss: 0.000210\n",
      "Epoch 196, LR: 6.25e-05, Train Loss: 0.000074, Val Loss: 0.000068\n",
      "Epoch 197, LR: 6.25e-05, Train Loss: 0.000070, Val Loss: 0.000080\n",
      "Epoch 198, LR: 6.25e-05, Train Loss: 0.000073, Val Loss: 0.000168\n",
      "Epoch 199, LR: 6.25e-05, Train Loss: 0.000072, Val Loss: 0.000078\n",
      "Epoch 200, LR: 3.13e-05, Train Loss: 0.000068, Val Loss: 0.000080\n",
      "Epoch 201, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000064\n",
      "Epoch 202, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000064\n",
      "Epoch 203, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000063\n",
      "Epoch 204, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000069\n",
      "Epoch 205, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000064\n",
      "Epoch 206, LR: 3.13e-05, Train Loss: 0.000064, Val Loss: 0.000063\n",
      "Epoch 207, LR: 3.13e-05, Train Loss: 0.000064, Val Loss: 0.000066\n",
      "Epoch 208, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000064\n",
      "Epoch 209, LR: 3.13e-05, Train Loss: 0.000064, Val Loss: 0.000067\n",
      "Epoch 210, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000064\n",
      "Epoch 211, LR: 3.13e-05, Train Loss: 0.000064, Val Loss: 0.000075\n",
      "Epoch 212, LR: 3.13e-05, Train Loss: 0.000064, Val Loss: 0.000065\n",
      "Epoch 213, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000063\n",
      "Epoch 214, LR: 3.13e-05, Train Loss: 0.000064, Val Loss: 0.000069\n",
      "Epoch 215, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000064\n",
      "Epoch 216, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000062\n",
      "Epoch 217, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000063\n",
      "Epoch 218, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000065\n",
      "Epoch 219, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000067\n",
      "Epoch 220, LR: 3.13e-05, Train Loss: 0.000063, Val Loss: 0.000066\n",
      "Epoch 221, LR: 3.13e-05, Train Loss: 0.000062, Val Loss: 0.000064\n",
      "Epoch 222, LR: 3.13e-05, Train Loss: 0.000062, Val Loss: 0.000061\n",
      "Epoch 223, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000061\n",
      "Epoch 224, LR: 3.13e-05, Train Loss: 0.000064, Val Loss: 0.000063\n",
      "Epoch 225, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000061\n",
      "Epoch 226, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000064\n",
      "Epoch 227, LR: 3.13e-05, Train Loss: 0.000062, Val Loss: 0.000062\n",
      "Epoch 228, LR: 3.13e-05, Train Loss: 0.000062, Val Loss: 0.000062\n",
      "Epoch 229, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000062\n",
      "Epoch 230, LR: 3.13e-05, Train Loss: 0.000062, Val Loss: 0.000062\n",
      "Epoch 231, LR: 3.13e-05, Train Loss: 0.000062, Val Loss: 0.000066\n",
      "Epoch 232, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000061\n",
      "Epoch 233, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000060\n",
      "Epoch 234, LR: 3.13e-05, Train Loss: 0.000060, Val Loss: 0.000062\n",
      "Epoch 235, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000061\n",
      "Epoch 236, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000070\n",
      "Epoch 237, LR: 3.13e-05, Train Loss: 0.000062, Val Loss: 0.000060\n",
      "Epoch 238, LR: 3.13e-05, Train Loss: 0.000060, Val Loss: 0.000060\n",
      "Epoch 239, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000062\n",
      "Epoch 240, LR: 3.13e-05, Train Loss: 0.000060, Val Loss: 0.000061\n",
      "Epoch 241, LR: 3.13e-05, Train Loss: 0.000060, Val Loss: 0.000072\n",
      "Epoch 242, LR: 3.13e-05, Train Loss: 0.000060, Val Loss: 0.000064\n",
      "Epoch 243, LR: 3.13e-05, Train Loss: 0.000061, Val Loss: 0.000061\n",
      "Epoch 244, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000059\n",
      "Epoch 245, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000067\n",
      "Epoch 246, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000059\n",
      "Epoch 247, LR: 3.13e-05, Train Loss: 0.000060, Val Loss: 0.000065\n",
      "Epoch 248, LR: 3.13e-05, Train Loss: 0.000062, Val Loss: 0.000062\n",
      "Epoch 249, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000059\n",
      "Epoch 250, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000060\n",
      "Epoch 251, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000065\n",
      "Epoch 252, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000058\n",
      "Epoch 253, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000060\n",
      "Epoch 254, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000060\n",
      "Epoch 255, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000062\n",
      "Epoch 256, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000060\n",
      "Epoch 257, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000092\n",
      "Epoch 258, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000058\n",
      "Epoch 259, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000065\n",
      "Epoch 260, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000058\n",
      "Epoch 261, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000060\n",
      "Epoch 262, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000063\n",
      "Epoch 263, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000061\n",
      "Epoch 264, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000059\n",
      "Epoch 265, LR: 3.13e-05, Train Loss: 0.000057, Val Loss: 0.000060\n",
      "Epoch 266, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000063\n",
      "Epoch 267, LR: 3.13e-05, Train Loss: 0.000059, Val Loss: 0.000059\n",
      "Epoch 268, LR: 3.13e-05, Train Loss: 0.000057, Val Loss: 0.000063\n",
      "Epoch 269, LR: 3.13e-05, Train Loss: 0.000057, Val Loss: 0.000056\n",
      "Epoch 270, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000057\n",
      "Epoch 271, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000056\n",
      "Epoch 272, LR: 3.13e-05, Train Loss: 0.000057, Val Loss: 0.000059\n",
      "Epoch 273, LR: 3.13e-05, Train Loss: 0.000057, Val Loss: 0.000058\n",
      "Epoch 274, LR: 3.13e-05, Train Loss: 0.000060, Val Loss: 0.000056\n",
      "Epoch 275, LR: 3.13e-05, Train Loss: 0.000057, Val Loss: 0.000056\n",
      "Epoch 276, LR: 3.13e-05, Train Loss: 0.000057, Val Loss: 0.000057\n",
      "Epoch 277, LR: 3.13e-05, Train Loss: 0.000056, Val Loss: 0.000060\n",
      "Epoch 278, LR: 3.13e-05, Train Loss: 0.000058, Val Loss: 0.000059\n",
      "Epoch 279, LR: 3.13e-05, Train Loss: 0.000056, Val Loss: 0.000056\n",
      "Epoch 280, LR: 3.13e-05, Train Loss: 0.000056, Val Loss: 0.000058\n",
      "Epoch 281, LR: 3.13e-05, Train Loss: 0.000057, Val Loss: 0.000056\n",
      "Epoch 282, LR: 1.56e-05, Train Loss: 0.000056, Val Loss: 0.000060\n",
      "Epoch 283, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000054\n",
      "Epoch 284, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 285, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000056\n",
      "Epoch 286, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 287, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000056\n",
      "Epoch 288, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 289, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 290, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 291, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 292, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 293, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000054\n",
      "Epoch 294, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000053\n",
      "Epoch 295, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 296, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000054\n",
      "Epoch 297, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000056\n",
      "Epoch 298, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000054\n",
      "Epoch 299, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000055\n",
      "Epoch 300, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000057\n",
      "Epoch 301, LR: 1.56e-05, Train Loss: 0.000054, Val Loss: 0.000055\n",
      "Epoch 302, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000053\n",
      "Epoch 303, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000055\n",
      "Epoch 304, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000053\n",
      "Epoch 305, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000054\n",
      "Epoch 306, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000053\n",
      "Epoch 307, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000053\n",
      "Epoch 308, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000054\n",
      "Epoch 309, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000053\n",
      "Epoch 310, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000054\n",
      "Epoch 311, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000056\n",
      "Epoch 312, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000054\n",
      "Epoch 313, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000055\n",
      "Epoch 314, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000052\n",
      "Epoch 315, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000052\n",
      "Epoch 316, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000053\n",
      "Epoch 317, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000052\n",
      "Epoch 318, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000056\n",
      "Epoch 319, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000055\n",
      "Epoch 320, LR: 1.56e-05, Train Loss: 0.000053, Val Loss: 0.000055\n",
      "Epoch 321, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000054\n",
      "Epoch 322, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000052\n",
      "Epoch 323, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000054\n",
      "Epoch 324, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000053\n",
      "Epoch 325, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000052\n",
      "Epoch 326, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000052\n",
      "Epoch 327, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000053\n",
      "Epoch 328, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000052\n",
      "Epoch 329, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000053\n",
      "Epoch 330, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000053\n",
      "Epoch 331, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000052\n",
      "Epoch 332, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000054\n",
      "Epoch 333, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000051\n",
      "Epoch 334, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000055\n",
      "Epoch 335, LR: 1.56e-05, Train Loss: 0.000052, Val Loss: 0.000052\n",
      "Epoch 336, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000052\n",
      "Epoch 337, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000053\n",
      "Epoch 338, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000053\n",
      "Epoch 339, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000052\n",
      "Epoch 340, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000051\n",
      "Epoch 341, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000051\n",
      "Epoch 342, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000052\n",
      "Epoch 343, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000052\n",
      "Epoch 344, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000053\n",
      "Epoch 345, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000051\n",
      "Epoch 346, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000051\n",
      "Epoch 347, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000051\n",
      "Epoch 348, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000055\n",
      "Epoch 349, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000053\n",
      "Epoch 350, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000050\n",
      "Epoch 351, LR: 1.56e-05, Train Loss: 0.000051, Val Loss: 0.000051\n",
      "Epoch 352, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000052\n",
      "Epoch 353, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000052\n",
      "Epoch 354, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000052\n",
      "Epoch 355, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000053\n",
      "Epoch 356, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000050\n",
      "Epoch 357, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000051\n",
      "Epoch 358, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000050\n",
      "Epoch 359, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000056\n",
      "Epoch 360, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000050\n",
      "Epoch 361, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000050\n",
      "Epoch 362, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000051\n",
      "Epoch 363, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000050\n",
      "Epoch 364, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000051\n",
      "Epoch 365, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000053\n",
      "Epoch 366, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000050\n",
      "Epoch 367, LR: 1.56e-05, Train Loss: 0.000049, Val Loss: 0.000050\n",
      "Epoch 368, LR: 1.56e-05, Train Loss: 0.000050, Val Loss: 0.000050\n",
      "Epoch 369, LR: 7.81e-06, Train Loss: 0.000049, Val Loss: 0.000050\n",
      "Epoch 370, LR: 7.81e-06, Train Loss: 0.000049, Val Loss: 0.000049\n",
      "Epoch 371, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 372, LR: 7.81e-06, Train Loss: 0.000049, Val Loss: 0.000049\n",
      "Epoch 373, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 374, LR: 7.81e-06, Train Loss: 0.000049, Val Loss: 0.000049\n",
      "Epoch 375, LR: 7.81e-06, Train Loss: 0.000049, Val Loss: 0.000050\n",
      "Epoch 376, LR: 7.81e-06, Train Loss: 0.000049, Val Loss: 0.000049\n",
      "Epoch 377, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 378, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000050\n",
      "Epoch 379, LR: 7.81e-06, Train Loss: 0.000049, Val Loss: 0.000049\n",
      "Epoch 380, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 381, LR: 7.81e-06, Train Loss: 0.000049, Val Loss: 0.000049\n",
      "Epoch 382, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 383, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 384, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 385, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 386, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 387, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 388, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 389, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 390, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 391, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 392, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 393, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 394, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 395, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 396, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 397, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 398, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 399, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000050\n",
      "Epoch 400, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 401, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 402, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 403, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 404, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 405, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 406, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 407, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 408, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 409, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000049\n",
      "Epoch 410, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 411, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 412, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 413, LR: 7.81e-06, Train Loss: 0.000048, Val Loss: 0.000048\n",
      "Epoch 414, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 415, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 416, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 417, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 418, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 419, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000049\n",
      "Epoch 420, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 421, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 422, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000049\n",
      "Epoch 423, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 424, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 425, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 426, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000049\n",
      "Epoch 427, LR: 7.81e-06, Train Loss: 0.000047, Val Loss: 0.000048\n",
      "Epoch 428, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 429, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 430, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 431, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 432, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 433, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 434, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 435, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 436, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 437, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 438, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 439, LR: 3.91e-06, Train Loss: 0.000047, Val Loss: 0.000047\n",
      "Epoch 440, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 441, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 442, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 443, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 444, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 445, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 446, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 447, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 448, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 449, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 450, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 451, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 452, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 453, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 454, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 455, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 456, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 457, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 458, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 459, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 460, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 461, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 462, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 463, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 464, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 465, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 466, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 467, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 468, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 469, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 470, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 471, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 472, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 473, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 474, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 475, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 476, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 477, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 478, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 479, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 480, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 481, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 482, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 483, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 484, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000047\n",
      "Epoch 485, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 486, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 487, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 488, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 489, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 490, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 491, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 492, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 493, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000046\n",
      "Epoch 494, LR: 3.91e-06, Train Loss: 0.000046, Val Loss: 0.000048\n",
      "Epoch 495, LR: 3.91e-06, Train Loss: 0.000045, Val Loss: 0.000046\n",
      "Epoch 496, LR: 3.91e-06, Train Loss: 0.000045, Val Loss: 0.000046\n",
      "Epoch 497, LR: 3.91e-06, Train Loss: 0.000045, Val Loss: 0.000046\n",
      "Epoch 498, LR: 3.91e-06, Train Loss: 0.000045, Val Loss: 0.000046\n",
      "Epoch 499, LR: 3.91e-06, Train Loss: 0.000045, Val Loss: 0.000046\n",
      "Epoch 500, LR: 3.91e-06, Train Loss: 0.000045, Val Loss: 0.000046\n",
      "Best model saved at: /projects/bcnx/kazumak2/MIMONet/HeatExchanger/checkpoints/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    train_model(\n",
    "        model_fn=MIMONet,\n",
    "        model_args=model_args,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_args={'lr': 1e-3, 'weight_decay': 1e-5},\n",
    "        scheduler_fn=scheduler_fn,\n",
    "        scheduler_args=scheduler_args,\n",
    "        dataset=train_dataset,\n",
    "        device=device,\n",
    "        num_epochs=500,\n",
    "        batch_size=4,\n",
    "        criterion=nn.MSELoss(),\n",
    "        patience=500,\n",
    "        k_fold=None,\n",
    "        multi_gpu=False,\n",
    "        working_dir=\"/projects/bcnx/kazumak2/MIMONet/HeatExchanger/\",\n",
    "    )\n",
    "\n",
    "# start the training\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3819ec6",
   "metadata": {},
   "source": [
    "### Evaluation with Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5fd9b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /projects/bcnx/kazumak2/MIMONet/HeatExchanger/checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MIMONet(\n",
       "  (branch_nets): ModuleList(\n",
       "    (0): FCN(\n",
       "      (network): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): FCN(\n",
       "      (network): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trunk_net): FCN(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MIMONet(**model_args).to(device)\n",
    "# Load the trained model\n",
    "model_path = os.path.join(working_dir, \"checkpoints/best_model.pt\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(\"Model loaded successfully from\", model_path)\n",
    "else:\n",
    "    print(\"Model file not found at\", model_path)\n",
    "    sys.exit(1)\n",
    "    \n",
    "# Evaluate the model on the test set\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "398fdc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the test loader to the model (and save predictions and targets)\n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        branch_data, trunk_data, target_data = batch\n",
    "        branch_data = [b.to(device) for b in branch_data]\n",
    "        trunk_data = trunk_data.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "\n",
    "        output = model(branch_data, trunk_data)\n",
    "        predictions.append(output.cpu().numpy())\n",
    "        targets.append(target_data.cpu().numpy())\n",
    "# Convert predictions and targets to numpy arrays\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "targets = np.concatenate(targets, axis=0)\n",
    "# Reverse the scaling for the target data\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "targets = scaler.inverse_transform(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3abee5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean relative L2 error per channel: [1.3565134 2.829426  1.6972986 0.9169749] %\n",
      "Standard deviation of relative L2 error per channel: [0.00113615 0.00031551 0.00032349 0.00051004]\n"
     ]
    }
   ],
   "source": [
    "# Compute L2 norm over grid points for each sample and channel\n",
    "l2_pred = np.linalg.norm(predictions, axis=1)  # shape: (samples, channels)\n",
    "l2_gt = np.linalg.norm(targets, axis=1)  # shape: (samples, channels)\n",
    "\n",
    "# Compute L2 error over grid points for each sample and channel\n",
    "l2_err = np.linalg.norm(predictions - targets, axis=1)  # shape: (samples, channels)\n",
    "\n",
    "# Compute relative error (avoid division by zero)\n",
    "rel_err = l2_err / (l2_gt + 1e-8)  # shape: (samples, channels)\n",
    "\n",
    "# Mean over samples for each channel\n",
    "mean_rel_err_per_channel = np.mean(rel_err, axis=0)  # shape: (channels,)\n",
    "\n",
    "print(\"Mean relative L2 error per channel:\", mean_rel_err_per_channel * 100, \"%\")\n",
    "\n",
    "# standard deviation of relative error per channel\n",
    "std_rel_err_per_channel = np.std(rel_err, axis=0)  # shape: (channels,)\n",
    "\n",
    "print(\"Standard deviation of relative L2 error per channel:\", std_rel_err_per_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b941233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, target.shape[-1], figsize=(15, 4))\n",
    "\n",
    "for i in range(target.shape[-1]):\n",
    "    ax[i].hist(rel_err[:, i] * 100, bins=50, density=True, alpha=0.7)\n",
    "    ax[i].set_title(f'{target_label[i]}')\n",
    "    ax[i].set_xlabel('Relative Error (%)')\n",
    "    ax[i].set_ylabel('Density')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd995f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch-env]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
