{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19563cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from utils import MIMONetDataset, DeepONetDataset, ChannelScaler\n",
    "from mimonet import MIMONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68626d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcf3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "working_dir = \"/projects/bcnx/kazumak2/MIMONet/HeatExchanger\"\n",
    "data_dir = os.path.join(working_dir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c3cb8",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662230f",
   "metadata": {},
   "source": [
    "### Load sharing parameters/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e26d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trunk dataset\n",
    "trunk_input = np.load(os.path.join(data_dir, \"share/trunk.npz\"))['trunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab5c36",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1bc97f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch1 shape: (1546, 2)\n",
      "Branch2 shape: (1546, 100)\n"
     ]
    }
   ],
   "source": [
    "# branch input dataset\n",
    "branch = np.load(os.path.join(data_dir, \"branch.npz\"))\n",
    "\n",
    "branch1 = branch['branch1']\n",
    "branch2 = branch['branch2']\n",
    "\n",
    "print(\"Branch1 shape:\", branch1.shape)\n",
    "print(\"Branch2 shape:\", branch2.shape)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(branch1))\n",
    "test_size = len(branch1) - train_size\n",
    "train_branch1, test_branch1 = branch1[:train_size], branch1[train_size:]\n",
    "train_branch2, test_branch2 = branch2[:train_size], branch2[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f02450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: (1546, 3977, 7)\n",
      "Train target shape: (1236, 3977, 7)\n",
      "Test target shape: (310, 3977, 7)\n"
     ]
    }
   ],
   "source": [
    "# output channels\n",
    "# 0: turb-kinetic-energy\n",
    "# 1: pressure\n",
    "# 2: temperature\n",
    "# 3: z-velocity\n",
    "# 4: y-velocity\n",
    "# 5: x-velocity\n",
    "# 6: velocity-magnitude\n",
    "\n",
    "# target dataset\n",
    "target = np.load(os.path.join(data_dir, \"target.npy\"))\n",
    "\n",
    "print(\"Target shape:\", target.shape)\n",
    "\n",
    "# split the target dataset into training and testing sets\n",
    "train_target = target[:train_size]\n",
    "test_target = target[train_size:]\n",
    "\n",
    "print(\"Train target shape:\", train_target.shape)\n",
    "print(\"Test target shape:\", test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85496e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of branch1: [  4.51454429 292.42944177]\n",
      "Std of branch1: [ 0.2615285  17.03323994]\n",
      "Mean of branch2: 12587.66968713018\n",
      "Std of branch2: 6302.709013835411\n"
     ]
    }
   ],
   "source": [
    "# (# train samples, 2) \n",
    "# get the mean and standard deviation of each channel\n",
    "mean_branch1 = np.mean(train_branch1, axis=0)\n",
    "std_branch1 = np.std(train_branch1, axis=0)\n",
    "\n",
    "print(\"Mean of branch1:\", mean_branch1)\n",
    "print(\"Std of branch1:\", std_branch1)\n",
    "\n",
    "# (# train samples, 100)\n",
    "mean_branch2 = np.mean(train_branch2)\n",
    "std_branch2 = np.std(train_branch2)\n",
    "\n",
    "print(\"Mean of branch2:\", mean_branch2)\n",
    "print(\"Std of branch2:\", std_branch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab08db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of normalized train_branch1: (1236, 2)\n",
      "Shape of normalized test_branch1: (310, 2)\n",
      "Shape of normalized train_branch2: (1236, 100)\n",
      "Shape of normalized test_branch2: (310, 100)\n"
     ]
    }
   ],
   "source": [
    "# normalize the branch data using the mean and std\n",
    "train_branch_1 = (train_branch1 - mean_branch1) / std_branch1\n",
    "test_branch_1 = (test_branch1 - mean_branch1) / std_branch1\n",
    "train_branch_2 = (train_branch2 - mean_branch2) / std_branch2\n",
    "test_branch_2 = (test_branch2 - mean_branch2) / std_branch2\n",
    "\n",
    "# print the shapes of the normalized data\n",
    "print(\"Shape of normalized train_branch1:\", train_branch_1.shape)\n",
    "print(\"Shape of normalized test_branch1:\", test_branch_1.shape)\n",
    "print(\"Shape of normalized train_branch2:\", train_branch_2.shape)\n",
    "print(\"Shape of normalized test_branch2:\", test_branch_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dddd92",
   "metadata": {},
   "source": [
    "### Scaling the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "688e7a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of scaled train_target: (1236, 3977, 7)\n",
      "Shape of scaled test_target: (310, 3977, 7)\n"
     ]
    }
   ],
   "source": [
    "# scaling the target data\n",
    "'''  \n",
    "note: reverse the scaling for the target data\n",
    "train_target = scaler.inverse_transform(train_target_scaled)\n",
    "test_target = scaler.inverse_transform(test_target_scaled)\n",
    "'''\n",
    "scaler = ChannelScaler(method='minmax', feature_range=(-1, 1))\n",
    "scaler.fit(train_target)\n",
    "train_target_scaled = scaler.transform(train_target)\n",
    "test_target_scaled = scaler.transform(test_target)\n",
    "\n",
    "print(\"Shape of scaled train_target:\", train_target_scaled.shape)\n",
    "print(\"Shape of scaled test_target:\", test_target_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87ae6a",
   "metadata": {},
   "source": [
    "## Torch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf71b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset and dataloader\n",
    "test_dataset = MIMONetDataset(\n",
    "    [test_branch_1, test_branch_2],  # branch_data_list\n",
    "    trunk_input,                     # trunk_data\n",
    "    test_target_scaled               # target_data\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # set to 1 for testing\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a754121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MIMONetDataset(\n",
    "    [train_branch_1, train_branch_2],  # branch_data_list\n",
    "    trunk_input,                       # trunk_data\n",
    "    train_target_scaled                # target_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b2674",
   "metadata": {},
   "source": [
    "## MIMONet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2abb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "dim = 256\n",
    "branch_input_dim1 = 2\n",
    "branch_input_dim2 = 100\n",
    "trunk_input_dim = 2\n",
    "\n",
    "# Define the model arguments for orig_MIMONet\n",
    "model_args = {\n",
    "    'branch_arch_list': [\n",
    "        [branch_input_dim1, 512, 512, 512, dim],\n",
    "        [branch_input_dim2, 512, 512, 512, dim]\n",
    "    ],\n",
    "    'trunk_arch': [trunk_input_dim, 256, 256, 256, dim],\n",
    "    'num_outputs': 7,\n",
    "    'activation_fn': nn.ReLU,\n",
    "    'merge_type': 'mul',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eadbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae939d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler parameters\n",
    "scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_args={'mode': 'min', 'factor': 0.5, 'patience': 10,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294932d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 1.00e-03, Train Loss: 0.100594, Val Loss: 0.096711\n",
      "Epoch 2, LR: 1.00e-03, Train Loss: 0.094217, Val Loss: 0.089276\n",
      "Epoch 3, LR: 1.00e-03, Train Loss: 0.077679, Val Loss: 0.060111\n",
      "Best model saved at: /projects/bcnx/kazumak2/MIMONet/HeatExchanger/checkpoints/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    train_model(\n",
    "        model_fn=MIMONet,\n",
    "        model_args=model_args,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_args={'lr': 1e-3, 'weight_decay': 1e-6},\n",
    "        scheduler_fn=scheduler_fn,\n",
    "        scheduler_args=scheduler_args,\n",
    "        dataset=train_dataset,\n",
    "        device=device,\n",
    "        num_epochs=3,\n",
    "        batch_size=4,\n",
    "        criterion=nn.MSELoss(),\n",
    "        patience=500,\n",
    "        k_fold=None,\n",
    "        multi_gpu=False,\n",
    "        working_dir=\"/projects/bcnx/kazumak2/MIMONet/HeatExchanger/\",\n",
    "    )\n",
    "\n",
    "# start the training\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3819ec6",
   "metadata": {},
   "source": [
    "### Evaluation with Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fd9b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /projects/bcnx/kazumak2/MIMONet/HeatExchanger/checkpoints/best_model.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MIMONet(\n",
       "  (branch_nets): ModuleList(\n",
       "    (0): FCN(\n",
       "      (network): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): FCN(\n",
       "      (network): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trunk_net): FCN(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=256, out_features=1792, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MIMONet(**model_args).to(device)\n",
    "# Load the trained model\n",
    "model_path = os.path.join(working_dir, \"checkpoints/best_model.pt\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(\"Model loaded successfully from\", model_path)\n",
    "else:\n",
    "    print(\"Model file not found at\", model_path)\n",
    "    sys.exit(1)\n",
    "    \n",
    "# Evaluate the model on the test set\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "398fdc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the test loader to the model (and save predictions and targets)\n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        branch_data, trunk_data, target_data = batch\n",
    "        branch_data = [b.to(device) for b in branch_data]\n",
    "        trunk_data = trunk_data.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "\n",
    "        output = model(branch_data, trunk_data)\n",
    "        predictions.append(output.cpu().numpy())\n",
    "        targets.append(target_data.cpu().numpy())\n",
    "# Convert predictions and targets to numpy arrays\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "targets = np.concatenate(targets, axis=0)\n",
    "# Reverse the scaling for the target data\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "targets = scaler.inverse_transform(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4660763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "#\n",
    "#y = trunk_input[:, 0]\n",
    "#z = trunk_input[:, 1]\n",
    "#\n",
    "## set the output channel\n",
    "#ch_id = 2\n",
    "#\n",
    "#\n",
    "#ax[0].scatter(y, z, c=targets[0, :, ch_id], cmap='viridis', s=1)\n",
    "## add colorbar\n",
    "#cbar = plt.colorbar(ax[0].collections[0], ax=ax[0])\n",
    "#\n",
    "#ax[1].scatter(y, z, c=test_target[0, :, ch_id], cmap='viridis', s=1)\n",
    "#cbar = plt.colorbar(ax[1].collections[0], ax=ax[1])\n",
    "#\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b941233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch-env]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
