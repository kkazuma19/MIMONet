{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c19563cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from utils import MIMONetDataset, DeepONetDataset, ChannelScaler\n",
    "from mimonet import MIMONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68626d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbcf3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "working_dir = \"/projects/bcnx/kazumak2/MIMONet/Subchannel/\"\n",
    "data_dir = os.path.join(working_dir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c3cb8",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662230f",
   "metadata": {},
   "source": [
    "### Load sharing parameters/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e26d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trunk dataset\n",
    "trunk_input = np.load(os.path.join(data_dir, \"share/trunk_input.npz\"))['trunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab5c36",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a28faea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_branch_1 shape: (4000, 100)\n",
      "train_branch_2 shape: (4000, 2)\n",
      "train_target shape: (4000, 1733, 3)\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "train_branch = np.load(os.path.join(data_dir, \"training/train_branch_input.npz\"))\n",
    "train_branch_1 = train_branch['func_params']\n",
    "train_branch_2 = train_branch['stat_params']\n",
    "\n",
    "# [samples, channel, gridpoints]\n",
    "train_target = np.load(os.path.join(data_dir, \"training/train_target.npz\"))['target']\n",
    "# convert to [samples, gridpoints, channel]\n",
    "train_target = np.moveaxis(train_target, 1, 2)\n",
    "\n",
    "print(\"train_branch_1 shape:\", train_branch_1.shape)\n",
    "print(\"train_branch_2 shape:\", train_branch_2.shape)\n",
    "print(\"train_target shape:\", train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc288a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the functional input data using predefined mean and std\n",
    "f_mean = np.load(os.path.join(data_dir, \"share/func_mean_std_params.npz\"))['mean']\n",
    "f_std = np.load(os.path.join(data_dir, \"share/func_mean_std_params.npz\"))['std']\n",
    "\n",
    "train_branch_1 = (train_branch_1 - f_mean) / f_std\n",
    "\n",
    "# scaling the static input data using predefined mean and std\n",
    "s_mean = np.load(os.path.join(data_dir, \"share/stat_mean_std_params.npz\"))['mean']\n",
    "s_std = np.load(os.path.join(data_dir, \"share/stat_mean_std_params.npz\"))['std']\n",
    "\n",
    "for i in range(s_mean.shape[0]):\n",
    "    train_branch_2[:, i] = (train_branch_2[:, i] - s_mean[i]) / s_std[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88f645",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54556d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_branch_1 shape: (1000, 100)\n",
      "test_branch_2 shape: (1000, 2)\n",
      "test_target shape: (1000, 1733, 3)\n"
     ]
    }
   ],
   "source": [
    "test_branch = np.load(os.path.join(data_dir, \"test/test_branch_input.npz\"))\n",
    "test_branch_1 = test_branch['func_params']\n",
    "test_branch_2 = test_branch['stat_params']\n",
    "\n",
    "test_target = np.load(os.path.join(data_dir, \"test/test_target.npz\"))['target']\n",
    "test_target = np.moveaxis(test_target, 1, 2)\n",
    "\n",
    "print(\"test_branch_1 shape:\", test_branch_1.shape)\n",
    "print(\"test_branch_2 shape:\", test_branch_2.shape)\n",
    "print(\"test_target shape:\", test_target.shape)\n",
    "\n",
    "# scaling the functional input data using predefined mean and std\n",
    "test_branch_1 = (test_branch_1 - f_mean) / f_std\n",
    "# scaling the static input data using predefined mean and std\n",
    "for i in range(s_mean.shape[0]):\n",
    "    test_branch_2[:, i] = (test_branch_2[:, i] - s_mean[i]) / s_std[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dddd92",
   "metadata": {},
   "source": [
    "### Scaling the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "688e7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the target data\n",
    "'''  \n",
    "note: reverse the scaling for the target data\n",
    "train_target = scaler.inverse_transform(train_target_scaled)\n",
    "test_target = scaler.inverse_transform(test_target_scaled)\n",
    "'''\n",
    "scaler = ChannelScaler(method='minmax', feature_range=(-1, 1))\n",
    "scaler.fit(train_target)\n",
    "train_target_scaled = scaler.transform(train_target)\n",
    "test_target_scaled = scaler.transform(test_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87ae6a",
   "metadata": {},
   "source": [
    "## Torch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf71b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset and dataloader\n",
    "test_dataset = MIMONetDataset(\n",
    "    [test_branch_1, test_branch_2],  # branch_data_list\n",
    "    trunk_input,                     # trunk_data\n",
    "    test_target_scaled               # target_data\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # set to 1 for testing\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a754121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MIMONetDataset(\n",
    "    [train_branch_1, train_branch_2],  # branch_data_list\n",
    "    trunk_input,                       # trunk_data\n",
    "    train_target_scaled                # target_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b2674",
   "metadata": {},
   "source": [
    "## MIMONet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57223d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,630,467\n"
     ]
    }
   ],
   "source": [
    "# Architecture parameters\n",
    "dim = 256\n",
    "branch_input_dim1 = 100\n",
    "branch_input_dim2 = 2\n",
    "trunk_input_dim = 2\n",
    "\n",
    "# Define MIONet instance (no Fourier, no final linear)\n",
    "model = MIMONet(\n",
    "    branch_arch_list=[\n",
    "        [branch_input_dim1, 512, 512, 512, dim],\n",
    "        [branch_input_dim2, 512, 512, 512, dim]\n",
    "    ],\n",
    "    trunk_arch=[trunk_input_dim, 256, 256, dim],\n",
    "    num_outputs=3, \n",
    "    activation_fn=nn.ReLU,\n",
    "    merge_type='mul'  # or 'sum'\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Print parameter count\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eadbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.training import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.194447, Val Loss: 0.191655\n",
      "Epoch 2, Train Loss: 0.178851, Val Loss: 0.138314\n",
      "Epoch 3, Train Loss: 0.128785, Val Loss: 0.121873\n",
      "Epoch 4, Train Loss: 0.114465, Val Loss: 0.110734\n",
      "Epoch 5, Train Loss: 0.110145, Val Loss: 0.128395\n",
      "Epoch 6, Train Loss: 0.108399, Val Loss: 0.137311\n",
      "Epoch 7, Train Loss: 0.107725, Val Loss: 0.110215\n",
      "Epoch 8, Train Loss: 0.106625, Val Loss: 0.108952\n",
      "Epoch 9, Train Loss: 0.107875, Val Loss: 0.110263\n",
      "Epoch 10, Train Loss: 0.105742, Val Loss: 0.127255\n",
      "Epoch 11, Train Loss: 0.105762, Val Loss: 0.107062\n",
      "Epoch 12, Train Loss: 0.104573, Val Loss: 0.106647\n",
      "Epoch 13, Train Loss: 0.103852, Val Loss: 0.104907\n",
      "Epoch 14, Train Loss: 0.102916, Val Loss: 0.107248\n",
      "Epoch 15, Train Loss: 0.101873, Val Loss: 0.103045\n",
      "Epoch 16, Train Loss: 0.100697, Val Loss: 0.105633\n",
      "Epoch 17, Train Loss: 0.098298, Val Loss: 0.096885\n",
      "Epoch 18, Train Loss: 0.095184, Val Loss: 0.096911\n",
      "Epoch 19, Train Loss: 0.092586, Val Loss: 0.096649\n",
      "Epoch 20, Train Loss: 0.091396, Val Loss: 0.087962\n",
      "Epoch 21, Train Loss: 0.088712, Val Loss: 0.087232\n",
      "Epoch 22, Train Loss: 0.085478, Val Loss: 0.082828\n",
      "Epoch 23, Train Loss: 0.084958, Val Loss: 0.087703\n",
      "Epoch 24, Train Loss: 0.083442, Val Loss: 0.143253\n",
      "Epoch 25, Train Loss: 0.081589, Val Loss: 0.109331\n",
      "Epoch 26, Train Loss: 0.087884, Val Loss: 0.089934\n",
      "Epoch 27, Train Loss: 0.081583, Val Loss: 0.101004\n",
      "Epoch 28, Train Loss: 0.078683, Val Loss: 0.077968\n",
      "Epoch 29, Train Loss: 0.077385, Val Loss: 0.074270\n",
      "Epoch 30, Train Loss: 0.077110, Val Loss: 0.075945\n",
      "Epoch 31, Train Loss: 0.076160, Val Loss: 0.072033\n",
      "Epoch 32, Train Loss: 0.075365, Val Loss: 0.073490\n",
      "Epoch 33, Train Loss: 0.074946, Val Loss: 0.071893\n",
      "Epoch 34, Train Loss: 0.074137, Val Loss: 0.073171\n",
      "Epoch 35, Train Loss: 0.072911, Val Loss: 0.075882\n",
      "Epoch 36, Train Loss: 0.074373, Val Loss: 0.081152\n",
      "Epoch 37, Train Loss: 0.071808, Val Loss: 0.092625\n",
      "Epoch 38, Train Loss: 0.074211, Val Loss: 0.068241\n",
      "Epoch 39, Train Loss: 0.072507, Val Loss: 0.069647\n",
      "Epoch 40, Train Loss: 0.071075, Val Loss: 0.069260\n",
      "Epoch 41, Train Loss: 0.071192, Val Loss: 0.067995\n",
      "Epoch 42, Train Loss: 0.068949, Val Loss: 0.066527\n",
      "Epoch 43, Train Loss: 0.070490, Val Loss: 0.065555\n",
      "Epoch 44, Train Loss: 0.069747, Val Loss: 0.065252\n",
      "Epoch 45, Train Loss: 0.068532, Val Loss: 0.075391\n",
      "Epoch 46, Train Loss: 0.067364, Val Loss: 0.065566\n",
      "Epoch 47, Train Loss: 0.067256, Val Loss: 0.066045\n",
      "Epoch 48, Train Loss: 0.066307, Val Loss: 0.065745\n",
      "Epoch 49, Train Loss: 0.065193, Val Loss: 0.109735\n",
      "Epoch 50, Train Loss: 0.066758, Val Loss: 0.087133\n",
      "Epoch 51, Train Loss: 0.064478, Val Loss: 0.090604\n",
      "Epoch 52, Train Loss: 0.066325, Val Loss: 0.066825\n",
      "Epoch 53, Train Loss: 0.063503, Val Loss: 0.062895\n",
      "Epoch 54, Train Loss: 0.062666, Val Loss: 0.072399\n",
      "Epoch 55, Train Loss: 0.063350, Val Loss: 0.066577\n",
      "Epoch 56, Train Loss: 0.061642, Val Loss: 0.065305\n",
      "Epoch 57, Train Loss: 0.063384, Val Loss: 0.059307\n",
      "Epoch 58, Train Loss: 0.060579, Val Loss: 0.061358\n",
      "Epoch 59, Train Loss: 0.060925, Val Loss: 0.070955\n",
      "Epoch 60, Train Loss: 0.060336, Val Loss: 0.084756\n",
      "Epoch 61, Train Loss: 0.059752, Val Loss: 0.061931\n",
      "Epoch 62, Train Loss: 0.059304, Val Loss: 0.058234\n",
      "Epoch 63, Train Loss: 0.059024, Val Loss: 0.056865\n",
      "Epoch 64, Train Loss: 0.058610, Val Loss: 0.069092\n",
      "Epoch 65, Train Loss: 0.059810, Val Loss: 0.056787\n",
      "Epoch 66, Train Loss: 0.057465, Val Loss: 0.055907\n",
      "Epoch 67, Train Loss: 0.057632, Val Loss: 0.084992\n",
      "Epoch 68, Train Loss: 0.057372, Val Loss: 0.064650\n",
      "Epoch 69, Train Loss: 0.057594, Val Loss: 0.057602\n",
      "Epoch 70, Train Loss: 0.055293, Val Loss: 0.081182\n",
      "Epoch 71, Train Loss: 0.058005, Val Loss: 0.053693\n",
      "Epoch 72, Train Loss: 0.056010, Val Loss: 0.056928\n",
      "Epoch 73, Train Loss: 0.053811, Val Loss: 0.058987\n",
      "Epoch 74, Train Loss: 0.055075, Val Loss: 0.054922\n",
      "Epoch 75, Train Loss: 0.054363, Val Loss: 0.052648\n",
      "Epoch 76, Train Loss: 0.053842, Val Loss: 0.049713\n",
      "Epoch 77, Train Loss: 0.054474, Val Loss: 0.051753\n",
      "Epoch 78, Train Loss: 0.051766, Val Loss: 0.049722\n",
      "Epoch 79, Train Loss: 0.050270, Val Loss: 0.050505\n",
      "Epoch 80, Train Loss: 0.050292, Val Loss: 0.048408\n",
      "Epoch 81, Train Loss: 0.052288, Val Loss: 0.048982\n",
      "Epoch 82, Train Loss: 0.049882, Val Loss: 0.049889\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSubchannel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# ✅ This creates Subchannel/checkpoints/\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/bcnx/kazumak2/MIMONet/scripts/training.py:109\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, device, num_epochs, batch_size, lr, patience, val_ratio, k_fold, multi_gpu, working_dir)\u001b[0m\n\u001b[1;32m    106\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 109\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m    112\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m/projects/bcnx/kazumak2/MIMONet/scripts/training.py:21\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), target_batch)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-env/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-env/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    device='cuda',\n",
    "    num_epochs=500,\n",
    "    batch_size=4,\n",
    "    lr=1e-3,\n",
    "    patience=10,\n",
    "    multi_gpu=False,\n",
    "    working_dir=\"Subchannel\"   # ✅ This creates Subchannel/checkpoints/\n",
    ")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch-env]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
